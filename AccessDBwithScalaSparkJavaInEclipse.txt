Accessing DB2 data from Apache Spark via standalone Scala and Java program in Eclipse using DB2 JDBC driver and DataFrames API
	* Java 1.8: java -version
	* Download DB2 JDBC driver: C:\labs\db2driver\db2jcc.jar
	* Download Spark on local machine from https://spark.apache.org/downloads.html
	  - download Spark pre-built binaries
		Spark release: 2.3.0
		package type: Pre-built for Apache Hadoop 2.7
		download type: Direct Download
		download Spark: spark-2.3.0-bin-hadoop2.7.tgz
	  - unzip it to a local directory: C:\labs\spark-2.3.0-bin-hadoop2.7
		tar xvzf spark-2.3.0-bin-hadoop2.7.tgz
	* Download windows native components (like winutils.exe, hadoop.dll etc) from https://github.com/srccodes/hadoop-common-2.2.0-bin/archive/master.zip
		extract it to C:\labs\spark-2.3.0-bin-hadoop2.7\bin directory
	* Download Scala Eclipse IDE from http://scala-ide.org/download/sdk.html
		Eclipse 4.7.1 (Oxygen)
		Scala IDE 4.7.0
		Scala 2.12.3 with Scala 2.11.11 and Scala 2.10.6
		Zinc 1.0.0
		Scala Worksheet 0.7.0
		ScalaTest 2.10.0.v-4-2_12
		Scala Refactoring 0.13.0
		Scala Search 0.6.0
		Scala IDE Play2 Plugin 0.10.0
		Scala IDE Lagom Plugin 1.0.0	
	* Launch Scala Eclipse IDE and create a new scala project
		- Java Build Path - add external JARs from C:\labs\spark-2.3.0-bin-hadoop2.7\jars
						    add external JARs from C:\labs\db2driver\db2jcc.jar
		- Scala Compiler - Scala Installation: Latest 2.11 bundle
		- Create new package scalatest
		  ** Create new scala object: DB2SparkScala 
		     DB2SparkScala.scala
				package scalatest

				import org.apache.spark.sql.SQLContext
				import org.apache.spark.SparkConf
				import org.apache.spark.SparkContext

				object DB2SparkScala extends App {
				  System.setProperty("hadoop.home.dir", "C:\\labs\\spark-2.3.0-bin-hadoop2.7\\")
				  val conf = new SparkConf()
					.setMaster("local[1]")
					.setAppName("GetEmployee")
					.set("spark.executor.memory", "1g")

				  val sc = new SparkContext(conf)
				  val sqlContext = new SQLContext(sc)

				  val employeeDF = sqlContext.load("jdbc", Map(
					"url" -> "jdbc:db2://192.168.3.120:50000/sample;user=db2user;password=db2user;",
					"driver" -> "com.ibm.db2.jcc.DB2Driver",
					"dbtable" -> "db2inst1.emp"))

				  employeeDF.show();
				}		  
				
		     Right click on DB2SparkScala.scala and select Run As Scala Application
		  
		  ** Create new Java class: DB2SparkJava
			 DB2SparkJava.java
				package scalatest;

				import org.apache.spark.sql.Dataset;
				import org.apache.spark.sql.Row;
				import org.apache.spark.sql.SparkSession;

				public class DB2SparkJava {
					public static void main(String[] args) {
						System.setProperty("hadoop.home.dir", "C:\\labs\\spark-2.3.0-bin-hadoop2.7\\");
						SparkSession sparkSession = SparkSession.builder()
								  .appName("My Spark Application")  // optional and will be autogenerated if not specified
								  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
								  .enableHiveSupport()              // self-explanatory, isn't it?
								  //.config("spark.sql.warehouse.dir", "target/spark-warehouse")
								  .getOrCreate();		

						Dataset<Row> data = sparkSession.read().format("jdbc")
							 .option("url", "jdbc:db2://192.168.3.120:50000/sample;")
							 .option("driver", "com.ibm.db2.jcc.DB2Driver")
							 .option("dbtable", "db2inst1.emp")
							 .option("user", "db2user")
							 .option("password", "db2user")
							 .load();

						System.out.println("count: " + data.count());

						data.show();		
					}
				}			 
			 
		     Right click on DB2SparkJava.java and select Run As Java Application
			 
	* For Maven project, link with Spark
		Spark artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:
				groupId: org.apache.spark
				artifactId: spark-core_2.11
				version: 2.3.0		
			 
			 