Apache Kafka Message Queue- distributed, fault tolerant, high throughput pub-sub messaging system.- written in Scala and java, runs on JVM- message broker + streaming system
- overview of Apache Kafka In a large distributed system, there are usually a lot of services that generate different events: logs, monitoring data, suspicious user actions, and 
so on.  In Kafka, these are called producers. On the other hand, there are services that need the generated data. These are called consumers.
 Kafka that solves the problem of interaction between these services. It sits between producers and consumers, while collecting the data from the 
producers,  storing them in a distributed repository of topics and giving each consumer the data by subscription.  Kafka is launched as a cluster consisting of one or several servers, each of which is called a broker.
 In other words, Kafka is a hybrid of a distributed database and a message queue. It is widely known for its features and is used by many large companies  to process terabytes of information. For example, in LinkedIn Apache Kafka is used to stream data about user activity, and Netflix uses it for data 
collection  and buffering for downstream systems like Elasticsearch, Amazon EMR, Mantis etc.
 - Kafka is a scalable, fault-tolerant messaging system. - Kafka is used for building real-time data pipelines and streaming applications.
 - Producers send messages to topic, which are records of streaming data. The topics exist on Kafka servers, also known as Brokers. - Consumers read the topic data from the brokers.  Producer --> Kafka Server / Broker : Topic --> Consumer - The topics are divided up into partitions, and it's these individual topic partitions that producers and consumers interact with. - Topic partitions are distributed throughout your cluster to balance load. This means that when producers and consumers interact with topic partitions,   they're sending and receiving from multiple brokers at the same time.  Producer --> Kafka Server / Broker : Topic --> Consumer           --> Kafka Server / Broker : Topic -->            --> Kafka Server / Broker : Topic -->  - Kafka supports replication natively. Each topic partition has a Replication Factor (RF) that determines the number of copies you have of your data.   For each replicated partition, only one broker is the leader. It's the leader of a partition that producers and consumers interact with.

- Apache Kafa - how to load test with JMeter https://www.blazemeter.com/blog/apache-kafka-how-to-load-test-with-jmeter
 Let’s look at some of Kafka’s features that are important for load testing: 1.A long message storing time - a week, by default 2.High performance, thanks to sequential I/O 3.Convenient clustering 4.High availability of data due to the capability to replicate and distribute queues across the cluster 5.Not only is it able to transfer data, but it can also process it by using Streaming API
 As a rule, Kafka is used to work with a very large amount of data. Therefore, the following aspects should be paid attention to during load testing:  1. Constantly writing the data to the disk will affect the capacity of the server. If insufficient, it will reach a denial of service state. 2. In addition, the distribution of sections and the number of brokers also affects the use of service capacity.     For example, brokers may simply not have enough resources to process the data stream.     As a result, the producers will exhaust local buffers for storing messages, and part of the messages may be lost. 3. When the replication feature is used, everything becomes even more complicated.     This is because its maintenance requires even more resources, and the case when brokers refuse to receive messages becomes even more possible.
 Data that is processed in such huge amounts, can be very easily lost, even though most processes are automated.  Therefore, testing of these services is important and it is necessary to be able to generate a proper load.
 To demonstrate load testing Apache Kafka, it will be installed on Ubuntu. Besides, we will use the Pepper-Box plugin as a producer.  It has a more convenient interface to work with message generation than kafkameter does.  We will have to implement the consumer on our own, because no plugin provides consumer implementation, and we are going to use the JSR223 Sampler to do 
that.
- Monitoring Kafka without losing your mind https://blog.newrelic.com/2017/12/12/new-relic-kafkapocalypse/
 - Pepper-Box: https://github.com/GSLabDev/pepper-box- kafameter: https://github.com/BrightTag/kafkameter- JSR223 Sampler: https://www.blazemeter.com/blog/beanshell-vs-jsr223-vs-java-jmeter-scripting-its-performance?
utm_source=blog&utm_medium=BM_blog&utm_campaign=apache-kafka-how-to-load-test-with-jmeter
- Apache Kafka  1. installation and setup     download and install Kafka: https://kafka.apache.org/quickstart %WORKDIR%\kafka_2.10-0.10.0.0  2. start a ZooKeeper server cd kafka_2.10-0.10.0.0 bin\windows\zookeeper-server-start.bat config\zookeeper.properties  3. start the Kafka server: cd kafka_2.10-0.10.0.0 bin\windows\kafka-server-start.bat config\server.properties  4. create a topic named "test" with a single partition and only one replica bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test      check the "test" topic by running the list topic command bin\windows\kafka-topics.bat --list --zookeeper localhost:2181
     It's possible to configure the brokers to auto-create topics when a non-existent topic is published to.
  5. send some messages with the command line producer client bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic test This is a message. This is a message.  6. start a consumer bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic test --from-beginning
 type messages in producer terminal and they will appear in consumer terminal
  7. setup a multi-broker cluster by starting two new nodes bin\windows\kafka-server-start.bat config\server1.properties bin\windows\kafka-server-start.bat config\server2.properties   8. create a new topic with a replication factor of three bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic test3
  9. run the "describe topics" command to show which broker is doing what in the cluster bin\windows\kafka-topics.bat --describe --zookeeper localhost:2181 --topic test3
 Topic:test3     PartitionCount:1        ReplicationFactor:3     Configs:        Topic: test3    Partition: 0    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1
     The first line gives a summary of all partitions.     Each additional line gives information about one partition. "Leader" is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the 
partitions. "Replicas" is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive. "isr" is the set of "in-sync' replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.
  10. publish a few messages to the new topic "test3" bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic test3 This is a message. This is a message.
      Consume the messages bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic test3 --from-beginning
  11. test out fault-tolerance. Broker 2 was acting as the leader so let's kill it.
 wmic process where "caption = 'java.exe' and commandline like '%server-2.properties%'" get processid taskkill /pid xxx /f
      Leadership has switched to one of the slaves and node 2 is no longer in the in-sync replicated-topic kafka-topics.bat --describe --zookeeper localhost:2181 --topic test3
 Topic:test3     PartitionCount:1        ReplicationFactor:3     Configs:        Topic: test3    Partition: 0    Leader: 0       Replicas: 2,0,1 Isr: 0,1
      but messages are still available for consumption even through the leader that tool the writes originally is down. bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic test3 --from-beginning
  12. use Kafka Connect to import/export data      Kafka Connect is a tool included with Kafka that imports and exports data to Kafka. It is an extensible tool that runs connectors,       which implement the custom logic for interacting with an external system. 
 echo foo> test.txt echo bar>> test.txt
      start two connectors running in standalone mode, which means they run in a single, local, dedicated process.  bin\windows\connect-standalone.bat config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
      We provide three configuration files as parameters.       - The first is always the configuration for the Kafka Connect process, containing common configuration         such as the Kafka brokers to connect to and the serialization format for data.       - The remaining configuration files each specify a connector to create. These files include a unique connector name,         the connector class to instantiate, and any other configuration required by the connector.
 These sample configuration files, included with Kafka, use the default local cluster configuration you started and create two connectors: the first is a source connector that reads lines from an input file and produces each to a Kafka topic and the second is a sink connector for that reads messages from a Kafka topic and produces each as a line in an output file.
      Once the Kafka Connect process has started, the source connector should start reading lines from test.txt       and producing them to the topic connect-test, and the sink connector should start reading messages from the topic connect-test       and write them to the file test.sink.txt. We can verify the data has been delivered through the entire pipeline by examining the contents       of the output file test.sink.txt.
      The data is being stored in the Kafka topic connect-test. We can run a console consumer to see the data in the topic: bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic connect-test --from-beginning
      The connectors continue to process data, so we can add data to the file and see it move through the pipeline:  echo Another line>> test.txt      You should see the line appear in the console consumer output and in the sink file.
  13. Use Kafka Streams to process data      Kafka Streams is a client library for building mission-critical real-time applications and microservices,       where the input and/or output data is stored in Kafka clusters. 
      Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side       with the benefits of Kafka's server-side cluster technology to make these applications highly scalable, elastic,       fault-tolerant, distributed, and 
much more. 
      This quickstart example (https://kafka.apache.org/10/documentation/streams/quickstart) will demonstrate how to run a streaming application coded in this 
library.  https://kafka.apache.org/10/documentation/streams/
      With Kafka Streams: - Elastic, highly scalable, fault-tolerant - Deploy to containers, VMs, bare metal, cloud - Equally viable for small, medium, & large use cases - Fully integrated with Kafka security - Write standard Java applications - Exactly-once processing semantics - No separate processing cluster required - Develop on Mac, Linux, Windows
 https://kafka.apache.org/10/documentation/streams/quickstart
      C:\TEMP\dcs302workspaces\kafka-demo  src\demo.kafka.WordCountApplicaiton.java
 - prepare the input topic: TextLinesTopic   bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TextLinesTopic
 - prepare the output topic: WordCountTopic   bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic WordCountTopic --config 
cleanup.policy=compact
   bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic WordCountTopic2
   Note: we create the output topic with compaction enabled because the output stream is a changelog stream.
 - The created topic can be described with the same kafka-topics tool:    bin\windows\kafka-topics.bat --zookeeper localhost:2181 --describe
   bin\windows\kafka-topics.bat --zookeeper localhost:2181 --list
 - start the app   bin\windows\kafka-run-class.bat demo.kafka.WordCountApplication
   The demo application will read from the input topic TextLinesTopic, perform the computations of the WordCount algorithm on each of the read messages,           and continuously write its current results to the output topic WordCountTopic. Hence there won't be any STDOUT output except log entries as the 
results are written back into in Kafka. 
 - start the console producer to write some input data to the input topic: TextLinesTopic  bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic TextLinesTopic
   and inspect the output of the demo.kafka.WordCountApplication by reading from its output topic WordCountTopic with the console consumerbin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic WordCountTopic --from-beginning --formatter 
kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property 
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

- Spring for Apache Kafka https://spring.io/blog/2017/12/01/spring-for-apache-kafka-2-1-0-release-and-1-3-2-2-0-2-available- Apache Kafka with Spring (https://projects.spring.io/spring-kafka/)
Spring for Apache Kafka - Tutorials  https://www.codenotfound.com/spring-kafka/1. Spring Kafka - Consumer Producer Example  https://www.codenotfound.com/spring-kafka-consumer-producer-example.html  step-by-step to implement an Apache Kafka Consumer and Producer using Spring Kafka and Spring Boot. 
  develop Kafka-based messaging solutions  provide a 'template' as a high-level abstraction for sending messages  support for Message-driven POJOs with @KafkaListener annotations and a listener container  
  Spring Kafka 1.1.7  Spring Boot 1.5.9  Maven 3.5
  Kafka Client Compatibility  https://projects.spring.io/spring-kafka/#kafka-client-compatibility
    * create the maven project with C:\TEMP\kafkademo1\pom.xml
  <dependencies>    <dependency>        <groupId>org.springframework.kafka</groupId>        <artifactId>spring-kafka</artifactId>        <version>1.1.7.RELEASE</version>    </dependency>  </dependencies>
  * create src\main\java\demo.kafka.SpringKafkaApplication.java  * create a sender and receiver https://www.codenotfound.com/spring-kafka-apache-avro-serializer-deserializer-example.html https://www.codenotfound.com/spring-kafka-json-serializer-deserializer-example.html
  * create src\main\resources\application.yml  * create a Spring Kafka Message Producer For sending messages we will be using the KafkaTemplate which wraps a Producer and provides convenience methods to send data to Kafka topics.  The template provides asynchronous send methods which return a ListenableFuture. src\main\java\demo.kafka.producer.Sender.java src\main\java\demo.kafka.producer.SenderConfig.java https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/producer/ProducerConfig.html    * create a Spring Kafka Message Consumer create a receiver that will handle the published messages.  The Receiver is nothing more than a simple POJO that defines a method for receiving messages.  In the example we named the method  receive() , but you can name it anything you like. 
 The @KafkaListener annotation creates a ConcurrentMessageListenerContainer message listener container behind the scenes for each annotated method.  In order to do so, a factory bean with name  kafkaListenerContainerFactory  is expected that we will configure.
 Using the  topics  element, we specify the topics for this listener. The name of the topic is injected from the  application.yml  properties file
 src\main\java\demo.kafka.consumer.Receiver.java src\main\java\demo.kafka.consumer.ReceiverConfig.java
 http://docs.spring.io/spring-kafka/api/org/springframework/kafka/annotation/KafkaListener.html https://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/consumer/ConsumerConfig.html
  * Testing the Spring Kafka Template & Listener   src\test\java\demo.kafka.SpringKafkaApplicationTest.java  to verify that we are able to send and recieve a message to and fom Apache Kafka.  An embedded Kafka broker is automatically started by using a  @ClassRule . https://www.codenotfound.com/spring-kafka-embedded-unit-test-example.html
  * run the test mvn test
 bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafkademo1.t
   bin\windows\kafka-topics.bat --zookeeper localhost:2181 --describe
   bin\windows\kafka-topics.bat --zookeeper localhost:2181 --list
 bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic kafkademo1.t This is a message. This is a message.
 bin\windows\kafka-console-consumer.bat --zookeeper localhost:2181 --bootstrap-server localhost:9092 --topic kafkademo1.t --from-beginning
2. Spring Kafka - Spring Boot Example  https://www.codenotfound.com/spring-kafka-boot-example.html how to setup Spring Kafka using Spring Boot autoconfiguration.   http://docs.spring.io/spring-boot/docs/current/reference/html/using-boot-auto-configuration.html
  Spring Boot auto-configuration attempts to automatically configure your Spring application based on the JAR dependencies that have been added.   In other words, if the spring-kafka-1.2.2.RELEASE.jar is on the classpath and you have not manually configured any Consumer or Provider beans,   then Spring Boot will auto-configure them using default values.
  * create the maven project with C:\TEMP\kafkademo2\pom.xml
  * autoconfigure the Spring Kafka Message Producer    The setup and creation of the  KafkaTemplate  and  Producer  beans is automatically done by Spring Boot.     The only things left to do are auto-wiring the  KafkaTemplate  and using it in the  send()  method.
    By annotating the  Sender  class with  @Component , Spring will instantiate this class as a bean that we will use in our test case.     In order for this to work, we also need the  @EnableAutoConfiguration  which was indirectly specified on  SpringKafkaApplication      by using the  @SpringBootApplication  annotation.
  * Autoconfigure the Spring Kafka Message Consumer    The setup and creation of the  ConcurrentKafkaListenerContainerFactory  and  KafkaMessageListenerContainer  beans is automatically done by Spring Boot.    The  @KafkaListener  annotation creates a message listener container for the annotated  receive()  method.     The topic name is specified using the  ${kafka.topic.boot}  placeholder for which the value will be automatically fetched     from the  application.yml  properties file.
    For the  Receiver, Spring Boot takes care of most of the configuration.     There are however two properties that need to be explicitly set in the  application.yml  properties file:    - The  kafka.consumer.auto-offset-reset  property needs to be set to  'earliest'  which ensures the new consumer group will       get the message sent in case the container started after the send was completed    - The  kafka.consumer.group-id  property needs to be specified as we are using group management to assign topic partitions to consumers.       In this example we will assign it the value  'boot' .
3. Spring Kafka - Batch Listener Example   https://www.codenotfound.com/spring-kafka-batch-listener-example.html
   Starting with Spring Kafka v1.1, @KafkaListener methods can be configured to receive a batch of consumer records from the consumer poll operation.
  * create the maven project with C:\TEMP\kafkademo3\pom.xml
  <dependencies>    <dependency>        <groupId>org.springframework.kafka</groupId>        <artifactId>spring-kafka</artifactId>        <version>1.2.2.RELEASE</version>    </dependency>  </dependencies>
  * create src\main\java\demo.kafka.SpringKafkaApplication.java  * create src\main\resources\application.yml  * create a Spring Kafka Message Producer src\main\java\demo.kafka.producer.Sender.java src\main\java\demo.kafka.producer.SenderConfig.java    * create a Spring Kafka Message Consumer create a receiver that will handle the published messages in batch
    Configuring a Batch Listener and Batch Size      Enabling batch receiving of messages can be achieved by setting the  batchListener  property.       This is done by calling the  setBatchListener()  method on the listener container factory       ( ConcurrentKafkaListenerContainerFactory in this example) with a value true.
      By default, the number of records received in each batch is dynamically calculated.       By setting the  'MAX_POLL_RECORDS_CONFIG'  property on the  ConsumerConfig  we can set an upper limit for the batch size.       For this example, we define a maximum of 10 messages to be returned per poll.
 src\main\java\demo.kafka.consumer.ReceiverConfig.java          public Map<String, Object> consumerConfigs() {     ...     // maximum records per poll, i.e. batch size     props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "10");
   public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> kafkaListenerContainerFactory() {     ...     // enable batch listening     factory.setBatchListener(true);
    The  receive()  method of the  Receiver  listener POJO needs to be updated to receive a  List  of payloads     (in this example these are simple  String  objects). Alternatively a list of  Message<?>  or  ConsumerRecord<?, ?>  objects can be configured.
    For logging purposes, we also add the partition and offset headers of each message to the  receive()  method.     These headers are available in a list and map to the received messages based on the index within the list.
    The  CountDownLatch  value that is used in the unit test case is increased so that we can send out a batch of 20 messages.
 src\main\java\demo.kafka.consumer.Receiver.java
   public static final int COUNT = 20;   private CountDownLatch latch = new CountDownLatch(COUNT);
   ...
   @KafkaListener(id = "batch-listener", topics = "${kafka.topic.kafkademo3}")   public void receive(List<String> data, @Header(KafkaHeaders.RECEIVED_PARTITION_ID) List<Integer> partitions,   @Header(KafkaHeaders.OFFSET) List<Long> offsets) {  LOGGER.info("start of batch receive");  for (int i = 0; i < data.size(); i++) {   LOGGER.info("received message='{}' with partition-offset='{}'", data.get(i),     partitions.get(i) + "-" + offsets.get(i));   // handle message
   latch.countDown();  }  LOGGER.info("end of batch receive");   }
 The @KafkaListener annotation creates a ConcurrentMessageListenerContainer message listener container behind the scenes for each annotated method.  In order to do so, a factory bean with name  kafkaListenerContainerFactory  is expected that we will configure.
 Using the  topics  element, we specify the topics for this listener. The name of the topic is injected from the  application.yml  properties file
  * Testing the Spring Kafka Template & Listener   src\test\java\demo.kafka.SpringKafkaApplicationTest.java  to verify that we are able to send and recieve a message to and fom Apache Kafka.  An embedded Kafka broker and ZooKeeper server is automatically started by using a JUnit @ClassRule .  https://www.codenotfound.com/spring-kafka-embedded-unit-test-example.html
 Using  @Before  we wait until all the partitions are assigned to our  Receiver  by looping over  the available  ConcurrentMessageListenerContainer  (if we don’t do this the message will already be sent  before the listeners are assigned to the topic).
 The  testReceive()  method uses a  for  loop to send out as many messages as were configured on the  CountDownLatch  in the  Receiver .  The result is that our listener starts receiving batches of messages from the Kafka broker partitions  (2 partitions are created by default on the embedded broker).
  * run the test mvn test

  * It's possible to schedule the listener to run on certain hours We could call stop on the KafkaListenerEndpointRegistry and it will stop all the Message Listener Containers. First get a reference to the registry:  @Autowired  private KafkaListenerEndpointRegistry kafkaListenerEndpointRegistry; Then call stop or start depending on when we want to run the listener:  kafkaListenerEndpointRegistry.stop();  kafkaListenerEndpointRegistry.start(); We could use Springs @Scheduled in order to run the above stop() and start().



  http://www.baeldung.com/spring-kafka
  Spring support for Kafka and the level of abstractions it provides over native Kafka Java client APIs.  Spring Kafka brings the simple and typical Spring template programming model with a KafkaTemplate and Message-driven POJOs via @KafkaListener annotation.

https://github.com/JohnReedLOL/kafka-streams/blob/master/src/main/java/io/confluent/examples/streams/WordCountLambdaExample.javahttps://raw.githubusercontent.com/JohnReedLOL/kafka-streams/master/src/main/java/io/confluent/examples/streams/WordCountLambdaExample.java
https://raw.githubusercontent.com/axbaretto/kafka/8906d865d129635da3d87f1b1eb85cb0af53f7c7/streams/src/main/java/org/apache/kafka/streams/kstream/KStream.java
Getting Started with Kafka Streams – building a streaming analytics Java application against a Kafka Topichttps://technology.amis.nl/2017/02/11/getting-started-with-kafka-streams-building-a-streaming-analytics-java-application-against-a-kafka-topic/
https://stackoverflow.com/questions/43742423/unsatisfiedlinkerror-on-lib-rocks-db-dll-when-developing-with-kafka-streamshttps://github.com/facebook/rocksdb/issues/1302 RocksDB on windows (librocksdbjni-win64.dll)- Can't find dependent libraries ==> find the missing dependencies by running Dependency Walker: http://stackoverflow.com/questions/6092200/how-to-fix-an-unsatisfiedlinkerror-cant-
find-dependent-libraries-in-a-jni-pro ==> need Visual C++ runtime for Visual Studio 2015 installed https://www.microsoft.com/en-us/download/details.aspx?id=48145 ==> opendedup http://opendedup.org/odd/overview/ ==> Kafka-stream v1.0.0 resolved the issue https://issues.apache.org/jira/browse/KAFKA-6167
https://www.chilkatsoft.com/java-loadlibrary-windows.asp How to Load a Java Native/Dynamic Library (DLL) -Djava.library.path=”C:\TEMP\lab2017\rocksdbjni-4.9.0;${env_var:PATH}”
Kestrel Message Queuehttps://twitter-archive.github.io/kestrel/- a simple distributed message queue written on JVM- each server handles a set of reliable, ordered message queues, with no cross communication, resulting in a cluster of k-ordered (loosely ordered) queues.- features: memcache protocol, thrift protocol, journaled (durable) queues, fanout queues (one writer, many readers), item expieration, transactional reads- clients can find kestrel servers via ZooKeeper- a working guide to kestrel https://twitter-archive.github.io/kestrel/docs/guide.html
 ZooKeeper

web server / reverse proxy server: IIS, Nginx, Apache
A reverse proxy server receives HTTP requests from Internet and forwards them to app server after some preliminary handling.
Kestrel web server implementation in ASP.NET Corehttps://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?tabs=aspnetcore2x

